{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Imdb_review.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZY9cyEI4CmXX",
        "colab_type": "code",
        "outputId": "a855ddf7-2641-46fc-8652-25123b9ac30f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "print(tf.__version__)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.0.0-rc0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JuquXCuRCzdY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow_datasets as tfdf"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UbrkeCdBC5Ed",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Imdm_dataet\n",
        "imdb,info = tfdf.load(\"imdb_reviews\",with_info = True,as_supervised = True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cHqkQVdQD_Sv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Split the dataset into train and test dataset\n",
        "import numpy as np\n",
        "train_data,test_data = imdb['train'],imdb['test']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q9LAdy8LEd8X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Converting the datasets into numpy array\n",
        "training_sentences = []\n",
        "training_labels = []\n",
        "testing_sentences = []\n",
        "testing_labels = []\n",
        "\n",
        "for s,l in train_data:\n",
        "  training_sentences.append(str(s.numpy()))\n",
        "  training_labels.append(l.numpy())\n",
        "  \n",
        "for s,l in test_data:\n",
        "  testing_sentences.append(str(s.numpy()))\n",
        "  testing_labels.append(l.numpy())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l7kQMjHiFh03",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#data preprocessing\n",
        "vocab_size = 10000\n",
        "embedding_dim  = 16\n",
        "max_length = 120\n",
        "trunc_type = 'post'\n",
        "oov_token = \"<00V>\"\n",
        "\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "tokenizer = Tokenizer(num_words = vocab_size , oov_token = oov_token)\n",
        "tokenizer.fit_on_texts(training_sentences)\n",
        "\n",
        "word_index = tokenizer.word_index\n",
        "sequences = tokenizer.texts_to_sequences(training_sentences)\n",
        "padded = pad_sequences(sequences,maxlen = max_length,truncating = trunc_type)\n",
        "\n",
        "testing_sequences = tokenizer.texts_to_sequences(testing_sentences)\n",
        "testing_padded = pad_sequences(testing_sequences,maxlen = max_length)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_fmTpC2KO5Y0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Converting the label dataset to numpy arrays\n",
        "testing_labels_final = np.array(testing_labels)\n",
        "training_labels_final = np.array(training_labels)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ZO8L3onRwaC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#To reverse the word index syntax from key,value pair to value,key pair for training\n",
        "reverse_word_index = dict([(value,key) for (key,value) in word_index.items()])\n",
        "\n",
        "def decode_review(text):\n",
        "  return ' '.join([reverse_word_index.get(i,'?') for i in text])\n",
        "print(decode_review(padded[1]))\n",
        "print(training_sentences[1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aXliGXsbJWAk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#define the model for neural network\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(vocab_size,embedding_dim,input_length = max_length),\n",
        "    tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.Dense(6, activation = 'relu'),\n",
        "    tf.keras.layers.Dense(1,activation = 'sigmoid')\n",
        "])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FKrCSwwFK1GT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Compile the model and printing the summary\n",
        "model.compile(loss='binary_crossentropy',optimizer='adam',metrics = ['accuracy'])\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hp9F5IqhLeFd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Define the checkpoints during training data\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "filepath=\"weights-improvement-{epoch:02d}-{loss:.4f}-bigger.hdf5\"\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
        "callbacks_list = [checkpoint]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UkyVANcVLufx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 751
        },
        "outputId": "8824c5ca-aed5-411d-dd67-07bda61d9f15"
      },
      "source": [
        "#training the data\n",
        "model.fit(padded,training_labels_final,epochs = 20,validation_data = (testing_padded,testing_labels_final))"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 25000 samples, validate on 25000 samples\n",
            "Epoch 1/20\n",
            "25000/25000 [==============================] - 5s 196us/sample - loss: 0.4952 - accuracy: 0.7397 - val_loss: 0.3497 - val_accuracy: 0.8450\n",
            "Epoch 2/20\n",
            "25000/25000 [==============================] - 4s 174us/sample - loss: 0.2438 - accuracy: 0.9050 - val_loss: 0.3741 - val_accuracy: 0.8357\n",
            "Epoch 3/20\n",
            "25000/25000 [==============================] - 4s 173us/sample - loss: 0.0974 - accuracy: 0.9733 - val_loss: 0.4451 - val_accuracy: 0.8268\n",
            "Epoch 4/20\n",
            "25000/25000 [==============================] - 4s 173us/sample - loss: 0.0253 - accuracy: 0.9968 - val_loss: 0.5290 - val_accuracy: 0.8215\n",
            "Epoch 5/20\n",
            "25000/25000 [==============================] - 4s 176us/sample - loss: 0.0058 - accuracy: 0.9996 - val_loss: 0.5995 - val_accuracy: 0.8228\n",
            "Epoch 6/20\n",
            "25000/25000 [==============================] - 4s 172us/sample - loss: 0.0018 - accuracy: 1.0000 - val_loss: 0.6482 - val_accuracy: 0.8240\n",
            "Epoch 7/20\n",
            "25000/25000 [==============================] - 4s 168us/sample - loss: 8.6807e-04 - accuracy: 1.0000 - val_loss: 0.6904 - val_accuracy: 0.8247\n",
            "Epoch 8/20\n",
            "25000/25000 [==============================] - 4s 167us/sample - loss: 4.6612e-04 - accuracy: 1.0000 - val_loss: 0.7293 - val_accuracy: 0.8256\n",
            "Epoch 9/20\n",
            "25000/25000 [==============================] - 4s 168us/sample - loss: 2.7357e-04 - accuracy: 1.0000 - val_loss: 0.7674 - val_accuracy: 0.8257\n",
            "Epoch 10/20\n",
            "25000/25000 [==============================] - 4s 168us/sample - loss: 1.6116e-04 - accuracy: 1.0000 - val_loss: 0.8048 - val_accuracy: 0.8258\n",
            "Epoch 11/20\n",
            "25000/25000 [==============================] - 4s 170us/sample - loss: 9.9208e-05 - accuracy: 1.0000 - val_loss: 0.8412 - val_accuracy: 0.8256\n",
            "Epoch 12/20\n",
            "25000/25000 [==============================] - 5s 182us/sample - loss: 6.1363e-05 - accuracy: 1.0000 - val_loss: 0.8776 - val_accuracy: 0.8250\n",
            "Epoch 13/20\n",
            "25000/25000 [==============================] - 4s 173us/sample - loss: 3.7868e-05 - accuracy: 1.0000 - val_loss: 0.9122 - val_accuracy: 0.8252\n",
            "Epoch 14/20\n",
            "25000/25000 [==============================] - 4s 176us/sample - loss: 2.3879e-05 - accuracy: 1.0000 - val_loss: 0.9468 - val_accuracy: 0.8252\n",
            "Epoch 15/20\n",
            "25000/25000 [==============================] - 4s 174us/sample - loss: 1.5319e-05 - accuracy: 1.0000 - val_loss: 0.9820 - val_accuracy: 0.8252\n",
            "Epoch 16/20\n",
            "25000/25000 [==============================] - 4s 176us/sample - loss: 9.6163e-06 - accuracy: 1.0000 - val_loss: 1.0171 - val_accuracy: 0.8245\n",
            "Epoch 17/20\n",
            "25000/25000 [==============================] - 4s 177us/sample - loss: 6.1040e-06 - accuracy: 1.0000 - val_loss: 1.0508 - val_accuracy: 0.8247\n",
            "Epoch 18/20\n",
            "25000/25000 [==============================] - 5s 195us/sample - loss: 3.9361e-06 - accuracy: 1.0000 - val_loss: 1.0842 - val_accuracy: 0.8253\n",
            "Epoch 19/20\n",
            "25000/25000 [==============================] - 4s 180us/sample - loss: 2.5155e-06 - accuracy: 1.0000 - val_loss: 1.1176 - val_accuracy: 0.8250\n",
            "Epoch 20/20\n",
            "25000/25000 [==============================] - 4s 178us/sample - loss: 1.6281e-06 - accuracy: 1.0000 - val_loss: 1.1509 - val_accuracy: 0.8251\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fad02934128>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5zU2B3fzN-wF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "3ef5d8d7-6f64-49ce-d2ef-9d1e7f661065"
      },
      "source": [
        "e = model.layers[0]\n",
        "weights = e.get_weights()[0]\n",
        "print(weights.shape)"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(10000, 16)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EbAYtCIMgBfN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Opening the vecs and meta files for Interpretaion of data\n",
        "import io\n",
        "out_v = io.open('vecs.tsv','w',encoding = 'utf-8')\n",
        "out_m = io.open('meta.tsv','w',encoding = 'utf-8')\n",
        "\n",
        "for word_num in range(1,vocab_size):\n",
        "  word = reverse_word_index[word_num]\n",
        "  embeddings = weights[word_num]\n",
        "  out_m.write(word + \"\\n\")\n",
        "  out_v.write('\\t'.join([str(x) for x in embeddings]) + \"\\n\")\n",
        "out_v.close()\n",
        "out_m.close()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}